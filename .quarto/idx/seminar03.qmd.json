{"title":"Seminar 03","markdown":{"yaml":{"title":"Seminar 03","subtitle":"MA22004","date":"2024-10-02","author":"Dr Eric Hall   â€¢   ehall001@dundee.ac.uk","format":{"revealjs":{"chalkboard":true,"html-math-method":"katex","theme":["default","resources/custom.scss"],"css":"resources/fonts.css","logo":"resources/logo.png","footer":"","template-partials":["resources/title-slide.html"],"transition":"slide","background-transition":"fade"}},"from":"markdown+emoji","lang":"en"},"headingText":"Announcements","headingAttr":{"id":"","classes":["mySegue","center"],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\nknitr::opts_chunk$set(echo = FALSE, comment = \"\", fig.asp = .5)\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(fontawesome)\nlibrary(latex2exp)\n```\n\n:::{.hidden}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\E}{\\mathbf{E}}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\newcommand{\\se}{\\mathsf{se}}\n\\DeclareMathOperator{\\sd}{sd}\n:::\n\n## Attendance\n\n::: {layout=\"[[-1], [1], [-1]]\"}\n![](images/seats.png){fig-align=\"center\" fig-alt=\"Register your attendance using SEAtS\"}\n:::\n\n## Reminders \n\n- Discuss labs, R, and RStudio at Thu workshop. \n- Discuss worksheet 2 at Fri workshop.\n- Lab 1 due **Fri 2024-10-04** at **17:00**. \n\n\n## Outline of today &nbsp;&nbsp;`r fa(\"compass\")`\n\n1. Overview of tools for drawing conclusions about the characteristics of a population from data:\n   -  point estimates\n   -  confidence intervals\n   -  hypothesis tests\n\n2. Why 0.05? &nbsp;&nbsp;`r fa(\"lightbulb\")`\n\n\n:::{.notes}\nOver next two seminars we will consider these concepts in general (to familiarize with language, notation, etc). \n\nThen we will focus on inferences in specific contexts. \n:::\n\n## Recall {.mySegue .center}\n\n## What is a statistic?\n\nA statistic is a quantity that can be calculated from sample data.\n\nPrior to obtaining data, a statistic is an unknown quantity and is therefore a random variable (rv).\n\n:::{.notes}\n- Let's look at some examples. \n:::\n\n## Examples of statistics\n\nViewed as *estimators* for characteristics of the population.\n\n- sample mean, $\\overline{X} = \\frac{1}{m} \\sum_{i=1}^m X_i$\n- sample variance, $S^2 = \\frac{1}{m-1} \\sum_{i=1}^m (X_i - \\overline{X})^2$\n- avg of extreme lengths, $\\widetilde{X} = \\frac{\\min_{i\\geq 0}(X_i) + \\max_{i\\geq 0}(X_i)}{2}$\n\n:::{.callout-warning}\n## Random variables have distributions\n\nAny function of rvs $X_i$ is also a rv and, therefore, has a probability distribution.\n:::\n\n## Sampling distributions\n\nWe refer to the probability distribution for a statistic as a sampling distribution to emphasize how the distribution will vary across all possible sample data.\n\n# What are point estimates? {.mySegue .center}\n\n## A point estimator\n\nConsider iid $X_1, X_2, \\dots, X_m \\sim F(\\theta)\\,.$ \n\nA point estimator $\\widehat{\\theta}_m$ of $\\theta$ is obtained by selecting a suitable statistic $g\\,,$\n$$\n  \\widehat{\\theta}_m = g(X_1, \\dots, X_m) \\,.\n$$\n\n:::{.notes}\n- Consider rvs $X_i \\sim F(\\theta)$ (general distribution)\n- Recall $\\theta$ a fixed, unknown quantity\n- NOTE $X_i$ ARE RVS! not observations (data points)\n:::\n\n## Closing the deal: point estimate\n\nA point estimate of a parameter $\\theta$ is a single number that we regard as a sensible value for $\\theta\\,.$ \n\nA point estimate $\\widehat{\\theta}_m$ is computed from an estimator using sample data.\n\n:::{.callout-warning}\n## Overloaded notation\n\nThe symbol $\\widehat{\\theta}_m$ is typically used to denote *both* the estimator and the point estimate resulting from a given sample. \n:::  \n\n:::{.notes}\n- Likewise, we refer to statistics: used for *both* the estimator and the estimate. \n- Writing $\\widehat{\\theta} = 42$ does not indicate how the point estimate was obtained; therefore, it is essential to report both the estimator and the resulting point estimate. \n- In addition, we should also report a measure of precision in our estimate. \n:::\n\n\n## Uncertainty in a point estimate\n\n- The standard error is one measure of the precision of an estimate. \n\n- The standard error of an estimator $\\widehat{\\theta}$ is the standard deviation:\n$$\n  \\sigma_{\\widehat{\\theta}} = \\sqrt{\\Var(\\widehat{\\theta})}\\,.\n$$\n\n- Estimated standard error is denoted by $\\widehat{\\sigma}_{\\widehat{\\theta}}$ or simply $s_{\\widehat{\\theta}}\\,.$\n\n:::{.notes}\n- Often, the standard error depends on unknown parameters and must also be estimated. \n- Standard error is sometimes denoted $\\mathsf{se} = \\mathsf{se}(\\widehat{\\theta})$ and the estimated standard error by $\\widehat{\\mathsf{se}}\\,.$\n:::\n\n\n# What are confidence intervals? {.mySegue .center}\n\n## A confidence interval\n\n- An interval estimate reports an entire range of plausible values for the parameter of interest. \n\n- A confidence interval (CI) is an interval estimate that makes a probability statement about the degree of reliability, or the confidence level, of the interval.\n\n:::{.notes}\n- Alternative to reporting a point estimate for a parameter!\n:::\n\n## CI definition {.smaller}\n\nA $100(1-\\alpha)\\%$ confidence interval for a parameter $\\theta$ is a *random* interval, $$C_m = (L_m , U_m)\\,,$$ where $L_m = \\ell(X_1, \\dots, X_m)$ and $U_m = u(X_1, \\dots, X_m)$ are functions of the data, such that,\n$$\n  P_{\\theta}(L_m < \\theta < U_m ) = 1 - \\alpha\\,, \n$$\nfor all $\\theta \\in \\Theta$ (the parameter space). \n\n:::{.notes}\n- The CI represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the $\\alpha$ level.\n-  If the true value of the parameter lies outside the $100(1-\\alpha)\\%$ confidence interval, then a sampling event has occurred (namely, obtaining a point estimate of the parameter at least this far from the true parameter value) which had a probability of $100\\alpha\\%$ (or less) of happening by chance. \n- Not a probability statement about the parameter $\\theta\\,.$ \n- Note that $\\theta$ is fixed ($\\theta$ is not a rv) and the interval $C_m$ is random. \n- After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or they do not. \n:::\n\n## What a  CI is not...\n\n> -  A $95\\%$ confidence level does not mean there is a $95\\%$ probability that the population parameter lies within a given interval.\n> -  A $95\\%$ confidence level does not mean that $95\\%$ of the sample data lie within the confidence interval.\n> - A particular confidence level of $95\\%$ calculated from an experiment does not mean that there is a $95\\%$ probability of a sample parameter from a repeat of the experiment falling within this interval.\n\n\n:::{.notes}\n- Nothing special about the number 95 here. \n- For 1st: i.e., a 95% probability that the interval covers the population parameter.\n- Once an interval is calculated, this interval either covers the parameter value or it does not!\n:::\n\n\n# What are hypothesis tests? {.mySegue .center}\n\n\n## Two hypotheses\n\nMethods for determining which of two contradictory claims, or *hypotheses*, about a parameter is correct. \n\n- $H_0$ the **null hypothesis** is a claim that we initially assume to be true by default. \n- $H_a$ the **alternative hypothesis** is an assertion that is contradictory to $H_0\\,.$ \n\n:::{.notes}\n- Typically, we shall consider hypothesis concerning a parameter $\\theta \\in \\Theta$ taking values in a parameter space. \n- The statistical hypothesis are contradictory in that $H_0$ and $H_a$ divide $\\Theta$ into two disjoint sets.\n:::\n\n## Hypothesis testing {.smaller}\n\n- A hypothesis test is a statistical procedure.\n\n- The test procedure is based on the initial assumption that $H_0$ is **true** and asks if the available data provides sufficient evidence to reject $H_0\\,.$ \n\n- If the observations disagree with $H_0\\,,$ then we reject the null hypothesis. On the other hand, if the sample evidence does not strongly contradict $H_0\\,,$ then we continue to believe $H_0\\,.$ \n\n:::{.callout-warning}\n## No \"accepting\" the null\nThe two possible conclusions of a hypothesis test are: **reject $H_0$** or **fail to reject $H_0\\,.$**\n:::\n\n:::{.notes}\n- *Fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*\n- Why not just *accept* the null and move on with our lives? Well, if I search the Highlands for the Scottish wildcat (critically endangered) and fail to find any, does that prove they do not exist?\n:::\n\n## Know when to fold 'em\n\n- What does evidence for a hypothesis test look like?\n\n- A test statistic $T$ is a function of the sample data (like an estimator). \n\n- The decision to reject or fail to reject $H_0$ will involve computing the test statistic.\n\n\n## How extreme is the evidence?\n\n- A rejection region $R$ is the collection of values of the test statistic for which $H_0$ is to be rejected in favor of the alternative: \n$$\nR = \\left\\{ x : T(x) > c \\right\\}\\,,\n$$\nwhere $c$ is referred to as a critical value. \n\n- For observed test statistic $t$, if $t \\in R$ then we reject $H_0\\,.$ The alternative is that $t \\not\\in R$ and in this case we fail to reject $H_0\\,.$ \n\n:::{.notes}\n- A procedure for carrying out a hypothesis test is based on specifying two additional items: \n  -  test statistic \n  -  corresponding rejection region.\n- If a given sample falls in the rejection region, then we reject $H_0\\,.$\n- If $X \\in R$ then the calculated test statistic exceeds some critical value.\n:::\n\n## Recall: critical values\n\nCritical values are quantiles of the reference distribution. \nCritical values are NOT areas.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\ndf <- 10\nL <- 30\nx1 <- qchisq(1-0.05/2, df=df, lower.tail = FALSE)\nx2 <- qchisq(0.05/2, df=df, lower.tail = FALSE)\nggplot(NULL, aes(c(0,L))) + \n  geom_line(stat = \"function\", fun = dchisq, args = list(df = df), xlim = c(0, L), linewidth = 1) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df = df), fill = \"blue\", xlim = c(0, x1), alpha = 0.3) + \n        geom_area(stat = \"function\", fun = dchisq, args = list(df = df), fill = \"blue\", xlim = c(x2, L), alpha = 0.3) +\n        annotate(\"text\", x = 10, y = .025, label=TeX(\"Each shaded area $=\\\\alpha$\"), color = \"blue\", size = 8) + \n        annotate(\"segment\", x = 10, xend = x1-1, y = 0.015, yend = .002, color = \"blue\", arrow=arrow(length=unit(0.2,\"cm\")), size = 2) + \n        annotate(\"segment\", x = 10, xend = x2+1, y = 0.015, yend = .002, color = \"blue\", arrow=arrow(length=unit(0.2,\"cm\")), size = 2) + \n        annotate(\"text\", x = x1, y = -0.01, label=TeX(\"$\\\\chi^2_{1-\\\\alpha, \\\\nu}$\"), linewidth = 2, size = 8) + \n        annotate(\"text\", x = x2, y = -0.01, label=TeX(\"$\\\\chi^2_{\\\\alpha, \\\\nu}$\"), linewidth = 2, size = 8) + \n        geom_segment(aes(x = x1, xend = x1, y = 0, yend = dchisq(x1, df = df)), linetype = 44) + \n        geom_segment(aes(x = x2, xend = x2, y = 0, yend = dchisq(x2, df = df)), linetype = 44) + ylim(-0.015, 0.1) + \n        theme(axis.title.y = element_blank(), axis.ticks.y=element_blank(), axis.text.y=element_blank(), axis.title.x = element_blank(), axis.ticks.x=element_blank(), axis.text.x=element_blank(), text = element_text(size = 20))\n```\n:::\n\n::: {.column width=\"40%\"}\n- Using R: `q`+distname, e.g., `qnorm`, `qt`, `qchisq`, ...\n- Using table\n:::\n::::\n\n:::{.notes}\n- $\\chi^2$ distribution is not symmetric! $\\mathsf{N}$ and $\\mathsf{t}$ are symmetric.\n- For $\\alpha/2 = 0.025\\,,$ $z_{\\alpha/2} = 1.96\\,.$\n:::\n\n## Balancing act\n\nThe basis for choosing a rejection region involves balancing **Type I** and **Type II** errors. A conclusion is reached in a hypothesis test by selecting a significance level $\\alpha$ for the test linked to the maximal type I error rate.\n\n-  Type I error occurs if $H_0$ is rejected when $H_0$ is actually true. \n-  Type II error is made if we fail to reject $H_0$ when $H_0$ is actually false.\n\n:::{.notes}\n-  Fix $\\alpha$ first!\n:::\n\n## How do $P$-values fit?\n\nA $P$-value is the probability, assuming $H_0$ is true, of obtaining a test statistic at least as contradictory to $H_0$ as the value calculated from the sample data.\n\nSmaller $P$-values indicate stronger evidence against $H_0\\,.$ \n\n- If $P \\leq \\alpha$ then we reject $H_0$ at significance level $\\alpha\\,.$ \n- If $P \\geq \\alpha$ we fail to reject $H_0$ at significance level $\\alpha\\,.$ \n\n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### One way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the $P$-value, probability calculated assuming the null is true of observing a test statistic value at least as contradictory to $H_0$ as what was obtained from data.\n3. For given $\\alpha\\,,$ reject $H_0$ if evidence is strong, i.e. $P < \\alpha\\,.$ If evidence is not strong enough, fail to reject $H_0\\,.$ \n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### Another way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the critical value for the level $\\alpha$ and hence identify the rejection region for the hypothesis test.\n3. If statistic falls in rejection region, reject $H_0$ because evidence is strong. If statistic falls outwith rejection region, we fail to reject $H_0$ because evidence is not strong enough. \n\n\n## Null distribution {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](images/htests.png){fig-alt=\"Pictoral representation of null distribution, with level and rejection region.\"}\n:::\n\n::: {.column width=\"40%\"}\nE.g., consider hypothesis tests for a population mean. \n\n- Distribution for $\\bar{X}$\n- Under the assumption that $H_0$ is true, so curve is centered at $\\mu_0$\n- Level $\\alpha$ is a probability (area)\n- Critical value $c$ is a point\n- Test statistic?\n:::\n::::\n\n# Why 0.05? {.mySegue .center}\n\n## Emily Dickinson and monkeys on the stair {.smaller}\n\n![Sarah Lee, Wikimedia Commons, CC-BY-SA 4.0](images/Batu_3.jpg){fig-alt=\"Photo of a monkey (Sarah Lee, Wikimedia Commons, CC BY-SA 4.0.)\"}\n\n\n\n## Personal significance level\n\nWhat is our MA22004 personal significance level?\n\n[www.openintro.org/book/stat/why05/](https://www.openintro.org/book/stat/why05/)\n\n\n# Summary\n\nToday we discussed: (point estimation), confidence intervals, and hypothesis tests. \n\nWe engaged in an activity to understand \"Why 0.05?\"\n\n:::{.callout-tip}\n## Today's materials \n\nSlides posted to <https://dundeemath.github.io/MA22004-seminar03>.\n:::","srcMarkdownNoYaml":"\n\n```{r}\n#| include: false\nknitr::opts_chunk$set(echo = FALSE, comment = \"\", fig.asp = .5)\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(fontawesome)\nlibrary(latex2exp)\n```\n\n# Announcements {.mySegue .center}\n:::{.hidden}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\E}{\\mathbf{E}}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\newcommand{\\se}{\\mathsf{se}}\n\\DeclareMathOperator{\\sd}{sd}\n:::\n\n## Attendance\n\n::: {layout=\"[[-1], [1], [-1]]\"}\n![](images/seats.png){fig-align=\"center\" fig-alt=\"Register your attendance using SEAtS\"}\n:::\n\n## Reminders \n\n- Discuss labs, R, and RStudio at Thu workshop. \n- Discuss worksheet 2 at Fri workshop.\n- Lab 1 due **Fri 2024-10-04** at **17:00**. \n\n\n## Outline of today &nbsp;&nbsp;`r fa(\"compass\")`\n\n1. Overview of tools for drawing conclusions about the characteristics of a population from data:\n   -  point estimates\n   -  confidence intervals\n   -  hypothesis tests\n\n2. Why 0.05? &nbsp;&nbsp;`r fa(\"lightbulb\")`\n\n\n:::{.notes}\nOver next two seminars we will consider these concepts in general (to familiarize with language, notation, etc). \n\nThen we will focus on inferences in specific contexts. \n:::\n\n## Recall {.mySegue .center}\n\n## What is a statistic?\n\nA statistic is a quantity that can be calculated from sample data.\n\nPrior to obtaining data, a statistic is an unknown quantity and is therefore a random variable (rv).\n\n:::{.notes}\n- Let's look at some examples. \n:::\n\n## Examples of statistics\n\nViewed as *estimators* for characteristics of the population.\n\n- sample mean, $\\overline{X} = \\frac{1}{m} \\sum_{i=1}^m X_i$\n- sample variance, $S^2 = \\frac{1}{m-1} \\sum_{i=1}^m (X_i - \\overline{X})^2$\n- avg of extreme lengths, $\\widetilde{X} = \\frac{\\min_{i\\geq 0}(X_i) + \\max_{i\\geq 0}(X_i)}{2}$\n\n:::{.callout-warning}\n## Random variables have distributions\n\nAny function of rvs $X_i$ is also a rv and, therefore, has a probability distribution.\n:::\n\n## Sampling distributions\n\nWe refer to the probability distribution for a statistic as a sampling distribution to emphasize how the distribution will vary across all possible sample data.\n\n# What are point estimates? {.mySegue .center}\n\n## A point estimator\n\nConsider iid $X_1, X_2, \\dots, X_m \\sim F(\\theta)\\,.$ \n\nA point estimator $\\widehat{\\theta}_m$ of $\\theta$ is obtained by selecting a suitable statistic $g\\,,$\n$$\n  \\widehat{\\theta}_m = g(X_1, \\dots, X_m) \\,.\n$$\n\n:::{.notes}\n- Consider rvs $X_i \\sim F(\\theta)$ (general distribution)\n- Recall $\\theta$ a fixed, unknown quantity\n- NOTE $X_i$ ARE RVS! not observations (data points)\n:::\n\n## Closing the deal: point estimate\n\nA point estimate of a parameter $\\theta$ is a single number that we regard as a sensible value for $\\theta\\,.$ \n\nA point estimate $\\widehat{\\theta}_m$ is computed from an estimator using sample data.\n\n:::{.callout-warning}\n## Overloaded notation\n\nThe symbol $\\widehat{\\theta}_m$ is typically used to denote *both* the estimator and the point estimate resulting from a given sample. \n:::  \n\n:::{.notes}\n- Likewise, we refer to statistics: used for *both* the estimator and the estimate. \n- Writing $\\widehat{\\theta} = 42$ does not indicate how the point estimate was obtained; therefore, it is essential to report both the estimator and the resulting point estimate. \n- In addition, we should also report a measure of precision in our estimate. \n:::\n\n\n## Uncertainty in a point estimate\n\n- The standard error is one measure of the precision of an estimate. \n\n- The standard error of an estimator $\\widehat{\\theta}$ is the standard deviation:\n$$\n  \\sigma_{\\widehat{\\theta}} = \\sqrt{\\Var(\\widehat{\\theta})}\\,.\n$$\n\n- Estimated standard error is denoted by $\\widehat{\\sigma}_{\\widehat{\\theta}}$ or simply $s_{\\widehat{\\theta}}\\,.$\n\n:::{.notes}\n- Often, the standard error depends on unknown parameters and must also be estimated. \n- Standard error is sometimes denoted $\\mathsf{se} = \\mathsf{se}(\\widehat{\\theta})$ and the estimated standard error by $\\widehat{\\mathsf{se}}\\,.$\n:::\n\n\n# What are confidence intervals? {.mySegue .center}\n\n## A confidence interval\n\n- An interval estimate reports an entire range of plausible values for the parameter of interest. \n\n- A confidence interval (CI) is an interval estimate that makes a probability statement about the degree of reliability, or the confidence level, of the interval.\n\n:::{.notes}\n- Alternative to reporting a point estimate for a parameter!\n:::\n\n## CI definition {.smaller}\n\nA $100(1-\\alpha)\\%$ confidence interval for a parameter $\\theta$ is a *random* interval, $$C_m = (L_m , U_m)\\,,$$ where $L_m = \\ell(X_1, \\dots, X_m)$ and $U_m = u(X_1, \\dots, X_m)$ are functions of the data, such that,\n$$\n  P_{\\theta}(L_m < \\theta < U_m ) = 1 - \\alpha\\,, \n$$\nfor all $\\theta \\in \\Theta$ (the parameter space). \n\n:::{.notes}\n- The CI represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the $\\alpha$ level.\n-  If the true value of the parameter lies outside the $100(1-\\alpha)\\%$ confidence interval, then a sampling event has occurred (namely, obtaining a point estimate of the parameter at least this far from the true parameter value) which had a probability of $100\\alpha\\%$ (or less) of happening by chance. \n- Not a probability statement about the parameter $\\theta\\,.$ \n- Note that $\\theta$ is fixed ($\\theta$ is not a rv) and the interval $C_m$ is random. \n- After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or they do not. \n:::\n\n## What a  CI is not...\n\n> -  A $95\\%$ confidence level does not mean there is a $95\\%$ probability that the population parameter lies within a given interval.\n> -  A $95\\%$ confidence level does not mean that $95\\%$ of the sample data lie within the confidence interval.\n> - A particular confidence level of $95\\%$ calculated from an experiment does not mean that there is a $95\\%$ probability of a sample parameter from a repeat of the experiment falling within this interval.\n\n\n:::{.notes}\n- Nothing special about the number 95 here. \n- For 1st: i.e., a 95% probability that the interval covers the population parameter.\n- Once an interval is calculated, this interval either covers the parameter value or it does not!\n:::\n\n\n# What are hypothesis tests? {.mySegue .center}\n\n\n## Two hypotheses\n\nMethods for determining which of two contradictory claims, or *hypotheses*, about a parameter is correct. \n\n- $H_0$ the **null hypothesis** is a claim that we initially assume to be true by default. \n- $H_a$ the **alternative hypothesis** is an assertion that is contradictory to $H_0\\,.$ \n\n:::{.notes}\n- Typically, we shall consider hypothesis concerning a parameter $\\theta \\in \\Theta$ taking values in a parameter space. \n- The statistical hypothesis are contradictory in that $H_0$ and $H_a$ divide $\\Theta$ into two disjoint sets.\n:::\n\n## Hypothesis testing {.smaller}\n\n- A hypothesis test is a statistical procedure.\n\n- The test procedure is based on the initial assumption that $H_0$ is **true** and asks if the available data provides sufficient evidence to reject $H_0\\,.$ \n\n- If the observations disagree with $H_0\\,,$ then we reject the null hypothesis. On the other hand, if the sample evidence does not strongly contradict $H_0\\,,$ then we continue to believe $H_0\\,.$ \n\n:::{.callout-warning}\n## No \"accepting\" the null\nThe two possible conclusions of a hypothesis test are: **reject $H_0$** or **fail to reject $H_0\\,.$**\n:::\n\n:::{.notes}\n- *Fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*\n- Why not just *accept* the null and move on with our lives? Well, if I search the Highlands for the Scottish wildcat (critically endangered) and fail to find any, does that prove they do not exist?\n:::\n\n## Know when to fold 'em\n\n- What does evidence for a hypothesis test look like?\n\n- A test statistic $T$ is a function of the sample data (like an estimator). \n\n- The decision to reject or fail to reject $H_0$ will involve computing the test statistic.\n\n\n## How extreme is the evidence?\n\n- A rejection region $R$ is the collection of values of the test statistic for which $H_0$ is to be rejected in favor of the alternative: \n$$\nR = \\left\\{ x : T(x) > c \\right\\}\\,,\n$$\nwhere $c$ is referred to as a critical value. \n\n- For observed test statistic $t$, if $t \\in R$ then we reject $H_0\\,.$ The alternative is that $t \\not\\in R$ and in this case we fail to reject $H_0\\,.$ \n\n:::{.notes}\n- A procedure for carrying out a hypothesis test is based on specifying two additional items: \n  -  test statistic \n  -  corresponding rejection region.\n- If a given sample falls in the rejection region, then we reject $H_0\\,.$\n- If $X \\in R$ then the calculated test statistic exceeds some critical value.\n:::\n\n## Recall: critical values\n\nCritical values are quantiles of the reference distribution. \nCritical values are NOT areas.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n```{r}\n#| echo: false\n#| warning: false\n#| message: false\ndf <- 10\nL <- 30\nx1 <- qchisq(1-0.05/2, df=df, lower.tail = FALSE)\nx2 <- qchisq(0.05/2, df=df, lower.tail = FALSE)\nggplot(NULL, aes(c(0,L))) + \n  geom_line(stat = \"function\", fun = dchisq, args = list(df = df), xlim = c(0, L), linewidth = 1) +\n  geom_area(stat = \"function\", fun = dchisq, args = list(df = df), fill = \"blue\", xlim = c(0, x1), alpha = 0.3) + \n        geom_area(stat = \"function\", fun = dchisq, args = list(df = df), fill = \"blue\", xlim = c(x2, L), alpha = 0.3) +\n        annotate(\"text\", x = 10, y = .025, label=TeX(\"Each shaded area $=\\\\alpha$\"), color = \"blue\", size = 8) + \n        annotate(\"segment\", x = 10, xend = x1-1, y = 0.015, yend = .002, color = \"blue\", arrow=arrow(length=unit(0.2,\"cm\")), size = 2) + \n        annotate(\"segment\", x = 10, xend = x2+1, y = 0.015, yend = .002, color = \"blue\", arrow=arrow(length=unit(0.2,\"cm\")), size = 2) + \n        annotate(\"text\", x = x1, y = -0.01, label=TeX(\"$\\\\chi^2_{1-\\\\alpha, \\\\nu}$\"), linewidth = 2, size = 8) + \n        annotate(\"text\", x = x2, y = -0.01, label=TeX(\"$\\\\chi^2_{\\\\alpha, \\\\nu}$\"), linewidth = 2, size = 8) + \n        geom_segment(aes(x = x1, xend = x1, y = 0, yend = dchisq(x1, df = df)), linetype = 44) + \n        geom_segment(aes(x = x2, xend = x2, y = 0, yend = dchisq(x2, df = df)), linetype = 44) + ylim(-0.015, 0.1) + \n        theme(axis.title.y = element_blank(), axis.ticks.y=element_blank(), axis.text.y=element_blank(), axis.title.x = element_blank(), axis.ticks.x=element_blank(), axis.text.x=element_blank(), text = element_text(size = 20))\n```\n:::\n\n::: {.column width=\"40%\"}\n- Using R: `q`+distname, e.g., `qnorm`, `qt`, `qchisq`, ...\n- Using table\n:::\n::::\n\n:::{.notes}\n- $\\chi^2$ distribution is not symmetric! $\\mathsf{N}$ and $\\mathsf{t}$ are symmetric.\n- For $\\alpha/2 = 0.025\\,,$ $z_{\\alpha/2} = 1.96\\,.$\n:::\n\n## Balancing act\n\nThe basis for choosing a rejection region involves balancing **Type I** and **Type II** errors. A conclusion is reached in a hypothesis test by selecting a significance level $\\alpha$ for the test linked to the maximal type I error rate.\n\n-  Type I error occurs if $H_0$ is rejected when $H_0$ is actually true. \n-  Type II error is made if we fail to reject $H_0$ when $H_0$ is actually false.\n\n:::{.notes}\n-  Fix $\\alpha$ first!\n:::\n\n## How do $P$-values fit?\n\nA $P$-value is the probability, assuming $H_0$ is true, of obtaining a test statistic at least as contradictory to $H_0$ as the value calculated from the sample data.\n\nSmaller $P$-values indicate stronger evidence against $H_0\\,.$ \n\n- If $P \\leq \\alpha$ then we reject $H_0$ at significance level $\\alpha\\,.$ \n- If $P \\geq \\alpha$ we fail to reject $H_0$ at significance level $\\alpha\\,.$ \n\n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### One way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the $P$-value, probability calculated assuming the null is true of observing a test statistic value at least as contradictory to $H_0$ as what was obtained from data.\n3. For given $\\alpha\\,,$ reject $H_0$ if evidence is strong, i.e. $P < \\alpha\\,.$ If evidence is not strong enough, fail to reject $H_0\\,.$ \n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### Another way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the critical value for the level $\\alpha$ and hence identify the rejection region for the hypothesis test.\n3. If statistic falls in rejection region, reject $H_0$ because evidence is strong. If statistic falls outwith rejection region, we fail to reject $H_0$ because evidence is not strong enough. \n\n\n## Null distribution {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](images/htests.png){fig-alt=\"Pictoral representation of null distribution, with level and rejection region.\"}\n:::\n\n::: {.column width=\"40%\"}\nE.g., consider hypothesis tests for a population mean. \n\n- Distribution for $\\bar{X}$\n- Under the assumption that $H_0$ is true, so curve is centered at $\\mu_0$\n- Level $\\alpha$ is a probability (area)\n- Critical value $c$ is a point\n- Test statistic?\n:::\n::::\n\n# Why 0.05? {.mySegue .center}\n\n## Emily Dickinson and monkeys on the stair {.smaller}\n\n![Sarah Lee, Wikimedia Commons, CC-BY-SA 4.0](images/Batu_3.jpg){fig-alt=\"Photo of a monkey (Sarah Lee, Wikimedia Commons, CC BY-SA 4.0.)\"}\n\n\n\n## Personal significance level\n\nWhat is our MA22004 personal significance level?\n\n[www.openintro.org/book/stat/why05/](https://www.openintro.org/book/stat/why05/)\n\n\n# Summary\n\nToday we discussed: (point estimation), confidence intervals, and hypothesis tests. \n\nWe engaged in an activity to understand \"Why 0.05?\"\n\n:::{.callout-tip}\n## Today's materials \n\nSlides posted to <https://dundeemath.github.io/MA22004-seminar03>.\n:::"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"katex","slide-level":2,"to":"revealjs","filters":["shinylive"],"from":"markdown+emoji","css":["resources/fonts.css"],"output-file":"seminar03.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.554","auto-stretch":true,"revealjs-plugins":["revealjs-text-resizer"],"chalkboard":true,"theme":["default","resources/custom.scss"],"logo":"resources/logo.png","footer":"","template-partials":["resources/title-slide.html"],"transition":"slide","backgroundTransition":"fade","title":"Seminar 03","subtitle":"MA22004","date":"2024-10-02","author":"Dr Eric Hall   â€¢   ehall001@dundee.ac.uk"}}},"projectFormats":["revealjs"]}