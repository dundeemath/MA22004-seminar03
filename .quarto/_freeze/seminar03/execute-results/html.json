{
  "hash": "23415756c8178a5820dab30c31961965",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Seminar 03\"\nsubtitle: \"MA22004\"\ndate: \"2024-10-02\"\nauthor: \"Dr Eric Hall   â€¢   ehall001@dundee.ac.uk\"\nformat: \n  revealjs:\n    chalkboard: true\n    html-math-method: katex\n    theme: [default, resources/custom.scss]\n    css: resources/fonts.css\n    logo: resources/logo.png\n    footer: \"\"\n    template-partials:\n      - resources/title-slide.html\n    transition: slide\n    background-transition: fade\nfrom: markdown+emoji\nlang: en\n---\n\n\n\n\n# Announcements {.mySegue .center}\n:::{.hidden}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\E}{\\mathbf{E}}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\newcommand{\\se}{\\mathsf{se}}\n\\DeclareMathOperator{\\sd}{sd}\n:::\n\n## Attendance\n\n::: {layout=\"[[-1], [1], [-1]]\"}\n![](images/seats.png){fig-align=\"center\" fig-alt=\"Register your attendance using SEAtS\"}\n:::\n\n## Reminders \n\n- Discuss labs, R, and RStudio at Thu workshop. \n- Discuss worksheet 2 at Fri workshop.\n- Lab 1 due **Fri 2024-10-04** at **17:00**. \n\n\n## Outline of today &nbsp;&nbsp;`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M464 256A208 208 0 1 0 48 256a208 208 0 1 0 416 0zM0 256a256 256 0 1 1 512 0A256 256 0 1 1 0 256zm306.7 69.1L162.4 380.6c-19.4 7.5-38.5-11.6-31-31l55.5-144.3c3.3-8.5 9.9-15.1 18.4-18.4l144.3-55.5c19.4-7.5 38.5 11.6 31 31L325.1 306.7c-3.2 8.5-9.9 15.1-18.4 18.4zM288 256a32 32 0 1 0 -64 0 32 32 0 1 0 64 0z\"/></svg>`{=html}\n\n1. Overview of tools for drawing conclusions about the characteristics of a population from data:\n   -  point estimates\n   -  confidence intervals\n   -  hypothesis tests\n\n2. Why 0.05? &nbsp;&nbsp;`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;\"><path d=\"M297.2 248.9C311.6 228.3 320 203.2 320 176c0-70.7-57.3-128-128-128S64 105.3 64 176c0 27.2 8.4 52.3 22.8 72.9c3.7 5.3 8.1 11.3 12.8 17.7l0 0c12.9 17.7 28.3 38.9 39.8 59.8c10.4 19 15.7 38.8 18.3 57.5H109c-2.2-12-5.9-23.7-11.8-34.5c-9.9-18-22.2-34.9-34.5-51.8l0 0 0 0c-5.2-7.1-10.4-14.2-15.4-21.4C27.6 247.9 16 213.3 16 176C16 78.8 94.8 0 192 0s176 78.8 176 176c0 37.3-11.6 71.9-31.4 100.3c-5 7.2-10.2 14.3-15.4 21.4l0 0 0 0c-12.3 16.8-24.6 33.7-34.5 51.8c-5.9 10.8-9.6 22.5-11.8 34.5H226.4c2.6-18.7 7.9-38.6 18.3-57.5c11.5-20.9 26.9-42.1 39.8-59.8l0 0 0 0 0 0c4.7-6.4 9-12.4 12.7-17.7zM192 128c-26.5 0-48 21.5-48 48c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-44.2 35.8-80 80-80c8.8 0 16 7.2 16 16s-7.2 16-16 16zm0 384c-44.2 0-80-35.8-80-80V416H272v16c0 44.2-35.8 80-80 80z\"/></svg>`{=html}\n\n\n:::{.notes}\nOver next two seminars we will consider these concepts in general (to familiarize with language, notation, etc). \n\nThen we will focus on inferences in specific contexts. \n:::\n\n## Recall {.mySegue .center}\n\n## What is a statistic?\n\nA statistic is a quantity that can be calculated from sample data.\n\nPrior to obtaining data, a statistic is an unknown quantity and is therefore a random variable (rv).\n\n:::{.notes}\n- Let's look at some examples. \n:::\n\n## Examples of statistics\n\nViewed as *estimators* for characteristics of the population.\n\n- sample mean, $\\overline{X} = \\frac{1}{m} \\sum_{i=1}^m X_i$\n- sample variance, $S^2 = \\frac{1}{m-1} \\sum_{i=1}^m (X_i - \\overline{X})^2$\n- avg of extreme lengths, $\\widetilde{X} = \\frac{\\min_{i\\geq 0}(X_i) + \\max_{i\\geq 0}(X_i)}{2}$\n\n:::{.callout-warning}\n## Random variables have distributions\n\nAny function of rvs $X_i$ is also a rv and, therefore, has a probability distribution.\n:::\n\n## Sampling distributions\n\nWe refer to the probability distribution for a statistic as a sampling distribution to emphasize how the distribution will vary across all possible sample data.\n\n# What are point estimates? {.mySegue .center}\n\n## A point estimator\n\nConsider iid $X_1, X_2, \\dots, X_m \\sim F(\\theta)\\,.$ \n\nA point estimator $\\widehat{\\theta}_m$ of $\\theta$ is obtained by selecting a suitable statistic $g\\,,$\n$$\n  \\widehat{\\theta}_m = g(X_1, \\dots, X_m) \\,.\n$$\n\n:::{.notes}\n- Consider rvs $X_i \\sim F(\\theta)$ (general distribution)\n- Recall $\\theta$ a fixed, unknown quantity\n- NOTE $X_i$ ARE RVS! not observations (data points)\n:::\n\n## Closing the deal: point estimate\n\nA point estimate of a parameter $\\theta$ is a single number that we regard as a sensible value for $\\theta\\,.$ \n\nA point estimate $\\widehat{\\theta}_m$ is computed from an estimator using sample data.\n\n:::{.callout-warning}\n## Overloaded notation\n\nThe symbol $\\widehat{\\theta}_m$ is typically used to denote *both* the estimator and the point estimate resulting from a given sample. \n:::  \n\n:::{.notes}\n- Likewise, we refer to statistics: used for *both* the estimator and the estimate. \n- Writing $\\widehat{\\theta} = 42$ does not indicate how the point estimate was obtained; therefore, it is essential to report both the estimator and the resulting point estimate. \n- In addition, we should also report a measure of precision in our estimate. \n:::\n\n\n## Uncertainty in a point estimate\n\n- The standard error is one measure of the precision of an estimate. \n\n- The standard error of an estimator $\\widehat{\\theta}$ is the standard deviation:\n$$\n  \\sigma_{\\widehat{\\theta}} = \\sqrt{\\Var(\\widehat{\\theta})}\\,.\n$$\n\n- Estimated standard error is denoted by $\\widehat{\\sigma}_{\\widehat{\\theta}}$ or simply $s_{\\widehat{\\theta}}\\,.$\n\n:::{.notes}\n- Often, the standard error depends on unknown parameters and must also be estimated. \n- Standard error is sometimes denoted $\\mathsf{se} = \\mathsf{se}(\\widehat{\\theta})$ and the estimated standard error by $\\widehat{\\mathsf{se}}\\,.$\n:::\n\n\n# What are confidence intervals? {.mySegue .center}\n\n## A confidence interval\n\n- An interval estimate reports an entire range of plausible values for the parameter of interest. \n\n- A confidence interval (CI) is an interval estimate that makes a probability statement about the degree of reliability, or the confidence level, of the interval.\n\n:::{.notes}\n- Alternative to reporting a point estimate for a parameter!\n:::\n\n## CI definition {.smaller}\n\nA $100(1-\\alpha)\\%$ confidence interval for a parameter $\\theta$ is a *random* interval, $$C_m = (L_m , U_m)\\,,$$ where $L_m = \\ell(X_1, \\dots, X_m)$ and $U_m = u(X_1, \\dots, X_m)$ are functions of the data, such that,\n$$\n  P_{\\theta}(L_m < \\theta < U_m ) = 1 - \\alpha\\,, \n$$\nfor all $\\theta \\in \\Theta$ (the parameter space). \n\n:::{.notes}\n- The CI represents values for the population parameter for which the difference between the parameter and the observed estimate is not statistically significant at the $\\alpha$ level.\n-  If the true value of the parameter lies outside the $100(1-\\alpha)\\%$ confidence interval, then a sampling event has occurred (namely, obtaining a point estimate of the parameter at least this far from the true parameter value) which had a probability of $100\\alpha\\%$ (or less) of happening by chance. \n- Not a probability statement about the parameter $\\theta\\,.$ \n- Note that $\\theta$ is fixed ($\\theta$ is not a rv) and the interval $C_m$ is random. \n- After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or they do not. \n:::\n\n## What a  CI is not...\n\n> -  A $95\\%$ confidence level does not mean there is a $95\\%$ probability that the population parameter lies within a given interval.\n> -  A $95\\%$ confidence level does not mean that $95\\%$ of the sample data lie within the confidence interval.\n> - A particular confidence level of $95\\%$ calculated from an experiment does not mean that there is a $95\\%$ probability of a sample parameter from a repeat of the experiment falling within this interval.\n\n\n:::{.notes}\n- Nothing special about the number 95 here. \n- For 1st: i.e., a 95% probability that the interval covers the population parameter.\n- Once an interval is calculated, this interval either covers the parameter value or it does not!\n:::\n\n\n# What are hypothesis tests? {.mySegue .center}\n\n\n## Two hypotheses\n\nMethods for determining which of two contradictory claims, or *hypotheses*, about a parameter is correct. \n\n- $H_0$ the **null hypothesis** is a claim that we initially assume to be true by default. \n- $H_a$ the **alternative hypothesis** is an assertion that is contradictory to $H_0\\,.$ \n\n:::{.notes}\n- Typically, we shall consider hypothesis concerning a parameter $\\theta \\in \\Theta$ taking values in a parameter space. \n- The statistical hypothesis are contradictory in that $H_0$ and $H_a$ divide $\\Theta$ into two disjoint sets.\n:::\n\n## Hypothesis testing {.smaller}\n\n- A hypothesis test is a statistical procedure.\n\n- The test procedure is based on the initial assumption that $H_0$ is **true** and asks if the available data provides sufficient evidence to reject $H_0\\,.$ \n\n- If the observations disagree with $H_0\\,,$ then we reject the null hypothesis. On the other hand, if the sample evidence does not strongly contradict $H_0\\,,$ then we continue to believe $H_0\\,.$ \n\n:::{.callout-warning}\n## No \"accepting\" the null\nThe two possible conclusions of a hypothesis test are: **reject $H_0$** or **fail to reject $H_0\\,.$**\n:::\n\n:::{.notes}\n- *Fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*\n- Why not just *accept* the null and move on with our lives? Well, if I search the Highlands for the Scottish wildcat (critically endangered) and fail to find any, does that prove they do not exist?\n:::\n\n## Know when to fold 'em\n\n- What does evidence for a hypothesis test look like?\n\n- A test statistic $T$ is a function of the sample data (like an estimator). \n\n- The decision to reject or fail to reject $H_0$ will involve computing the test statistic.\n\n\n## How extreme is the evidence?\n\n- A rejection region $R$ is the collection of values of the test statistic for which $H_0$ is to be rejected in favor of the alternative: \n$$\nR = \\left\\{ x : T(x) > c \\right\\}\\,,\n$$\nwhere $c$ is referred to as a critical value. \n\n- For observed test statistic $t$, if $t \\in R$ then we reject $H_0\\,.$ The alternative is that $t \\not\\in R$ and in this case we fail to reject $H_0\\,.$ \n\n:::{.notes}\n- A procedure for carrying out a hypothesis test is based on specifying two additional items: \n  -  test statistic \n  -  corresponding rejection region.\n- If a given sample falls in the rejection region, then we reject $H_0\\,.$\n- If $X \\in R$ then the calculated test statistic exceeds some critical value.\n:::\n\n## Recall: critical values\n\nCritical values are quantiles of the reference distribution. \nCritical values are NOT areas.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](seminar03_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column width=\"40%\"}\n- Using R: `q`+distname, e.g., `qnorm`, `qt`, `qchisq`, ...\n- Using table\n:::\n::::\n\n:::{.notes}\n- $\\chi^2$ distribution is not symmetric! $\\mathsf{N}$ and $\\mathsf{t}$ are symmetric.\n- For $\\alpha/2 = 0.025\\,,$ $z_{\\alpha/2} = 1.96\\,.$\n:::\n\n## Balancing act\n\nThe basis for choosing a rejection region involves balancing **Type I** and **Type II** errors. A conclusion is reached in a hypothesis test by selecting a significance level $\\alpha$ for the test linked to the maximal type I error rate.\n\n-  Type I error occurs if $H_0$ is rejected when $H_0$ is actually true. \n-  Type II error is made if we fail to reject $H_0$ when $H_0$ is actually false.\n\n:::{.notes}\n-  Fix $\\alpha$ first!\n:::\n\n## How do $P$-values fit?\n\nA $P$-value is the probability, assuming $H_0$ is true, of obtaining a test statistic at least as contradictory to $H_0$ as the value calculated from the sample data.\n\nSmaller $P$-values indicate stronger evidence against $H_0\\,.$ \n\n- If $P \\leq \\alpha$ then we reject $H_0$ at significance level $\\alpha\\,.$ \n- If $P \\geq \\alpha$ we fail to reject $H_0$ at significance level $\\alpha\\,.$ \n\n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### One way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the $P$-value, probability calculated assuming the null is true of observing a test statistic value at least as contradictory to $H_0$ as what was obtained from data.\n3. For given $\\alpha\\,,$ reject $H_0$ if evidence is strong, i.e. $P < \\alpha\\,.$ If evidence is not strong enough, fail to reject $H_0\\,.$ \n\n## The skinny on hypothesis testing {.smaller}\n\n:::{.callout-warning}\n## Two routes to the same answer!  \nBut both are really the same. \n:::\n\n### Another way: \n\n1. Compute the value of an appropriate test statistic.\n2. Determine the critical value for the level $\\alpha$ and hence identify the rejection region for the hypothesis test.\n3. If statistic falls in rejection region, reject $H_0$ because evidence is strong. If statistic falls outwith rejection region, we fail to reject $H_0$ because evidence is not strong enough. \n\n\n## Null distribution {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"60%\"}\n![](images/htests.png){fig-alt=\"Pictoral representation of null distribution, with level and rejection region.\"}\n:::\n\n::: {.column width=\"40%\"}\nE.g., consider hypothesis tests for a population mean. \n\n- Distribution for $\\bar{X}$\n- Under the assumption that $H_0$ is true, so curve is centered at $\\mu_0$\n- Level $\\alpha$ is a probability (area)\n- Critical value $c$ is a point\n- Test statistic?\n:::\n::::\n\n# Why 0.05? {.mySegue .center}\n\n## Emily Dickinson and monkeys on the stair {.smaller}\n\n![Sarah Lee, Wikimedia Commons, CC-BY-SA 4.0](images/Batu_3.jpg){fig-alt=\"Photo of a monkey (Sarah Lee, Wikimedia Commons, CC BY-SA 4.0.)\"}\n\n\n\n## Personal significance level\n\nWhat is our MA22004 personal significance level?\n\n[www.openintro.org/book/stat/why05/](https://www.openintro.org/book/stat/why05/)\n\n\n# Summary\n\nToday we discussed: (point estimation), confidence intervals, and hypothesis tests. \n\nWe engaged in an activity to understand \"Why 0.05?\"\n\n:::{.callout-tip}\n## Today's materials \n\nSlides posted to <https://dundeemath.github.io/MA22004-seminar03>.\n:::",
    "supporting": [
      "seminar03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}