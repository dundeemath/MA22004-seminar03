---
title: "MA22004"
subtitle: "Seminar 03"
author: "Dr Eric Hall   â€¢   ehall001@dundee.ac.uk"
date: "2023-10-04"
output: 
 ioslides_presentation:
   widescreen: true
   css: ../assets/style.css
   logo: ../assets/images/rev_logo.png
---
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{\mathbf{E}}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\corr}{corr}
\newcommand{\se}{\mathsf{se}}
\DeclareMathOperator{\sd}{sd}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = "", fig.asp = .5)
library(tidyverse)
library(latex2exp)
library(knitr)
library(kableExtra)
library(janitor)
library(fontawesome)
library(latex2exp)

theme_ur <- theme(legend.justification = c(1,1), legend.position = c(1,1), legend.box.margin = margin(c(4, 4, 4, 4), unit = "pt"))
theme_lr <- theme(legend.justification = c(1,0), legend.position = c(1,0), legend.box.margin = margin(c(4, 4, 4, 4), unit = "pt"))
theme_ul <- theme(legend.justification = c(0,1), legend.position = c(0,1), legend.box.margin = margin(c(4, 4, 4, 4), unit = "pt"))
options(width = 90, knitr.kable.NA = '', "kableExtra.html.bsTable" = T)
lsz <- 1.0
tsz <- 4
```

## {.mySegue .flexbox .vcenter}

:::{.checkinblock}
Please register your attendance using SEAtS. 
:::

## Announcements &nbsp;&nbsp;`r fa("bullhorn")`

### Reminders 

- Discuss labs, R, and RStudio at Thu workshop. 
- Discuss worksheet 2 at Fri workshop.
- Lab 1 due **Fri 2023-10-06** at **17:00**. 

## Outline of today &nbsp;&nbsp;`r fa("compass")`

1. Today we will finish our overview of tools for drawing conclusions about the characteristics of a population from data:
   -  ~~point estimation~~
   -  ~~confidence intervals~~
   -  hypothesis testing

2. Why 0.05? &nbsp;&nbsp;`r fa("lightbulb")`

3. Inferences for population means


## {.mySegue .flexbox .vcenter}
Hypothesis testing 

## Two hypotheses

Methods for determining which of two contradictory claims, or **hypotheses**, about a parameter is correct. 

- $H_0$ the **null hypothesis** is a claim that we initially assume to be true by default. 
- $H_a$ the **alternative hypothesis** is an assertion that is contradictory to $H_0$. 

<div class="notes">
- Typically, we shall consider hypothesis concerning a parameter $\theta \in \Theta$ taking values in a parameter space. 
- The statistical hypothesis are contradictory in that $H_0$ and $H_a$ divide $\Theta$ into two disjoint sets.
</div>

## Why not *hypotheses* test?

A **hypothesis test** is a statistical procedure.

The test procedure is based on the initial assumption that $H_0$ is true.

H-test asks if the available data provides sufficient evidence to reject the null hypothesis $H_0$. 

If the observations disagree with $H_0$, then we reject the null hypothesis. On the other hand, if the sample evidence does not strongly contradict $H_0$, then we continue to believe $H_0$. 

:::{.warningblock}
The two possible conclusions of a hypothesis test are: *reject $H_0$* or *fail to reject $H_0$*.
:::

<div class="notes">
- *Fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*
- Why not just *accept* the null and move on with our lives? Well, if I search the Highlands for the Scottish wildcat (critically endangered) and fail to find any, does that prove they do not exist?
</div>

## Know when to fold 'em

A **test statistic** $T$ is a function of the sample data (like an estimator). The decision to reject or fail to reject $H_0$ will involve computing the test statistic.


The **rejection region** $R$ is the collection of values of the test statistic for which $H_0$ is to be rejected in favor of the alternative, e.g.,
\[
R = \left\{ x : T(x) > c \right\}\,,
\]
where $c$ is referred to as a **critical value**. 
If $X \in R$ then we reject $H_0$. The alternative is that $X \not\in R$ and in this case we fail to reject the null. 

<div class="notes">
- A procedure for carrying out a hypothesis test is based on specifying two additional items: 
  -  test statistic 
  -  corresponding rejection region.
- If a given sample falls in the rejection region, then we reject $H_0$.
- If $X \in R$ then the calculated test statistic exceeds some critical value.
</div>

## Recall: critical values

Critical values are quantiles of the reference distribution. Critical values are NOT areas.

```{r network, echo=FALSE, fig.cap="", out.width = '50%'}
knitr::include_graphics("images/exm-htest-chisq-plot-1.png")
```

- Using R: `qnorm`, `qt`, `qchisq`
- Using table

<div class="notes">
- $\chi^2$ distribution is not symmetric! $\mathsf{N}$ and $\mathsf{t}$ are symmetric.
- For $\alpha/2 = 0.025$, $z_{\alpha/2} = 1.96$.
</div>

## Balancing act

The basis for choosing a rejection region involves balancing Type I and II errors. A conclusion is reached in a hypothesis test by selecting a **significance level** $\alpha$ for the test linked to the maximal type I error rate.

<div class="notes">
-  **Type I** error occurs if $H_0$ is rejected when $H_0$ is actually true. 
-  **Type II** error is made if we fail to reject $H_0$ when $H_0$ is actually false.
-  Fix $\alpha$ first!
</div>

## \(P\)-values (alternative to rejection region) 

A **$P$-value** is the probability, calculated assuming $H_0$ is true, of obtaining a value of the test statistic at least as contradictory to $H_0$ as the value calculated from the sample data.

Smaller $P$-values indicate stronger evidence against $H_0$ in favor of $H_a$. 

- If $P \leq \alpha$ then we reject $H_0$ at significance level $\alpha$. 
- If $P \geq \alpha$ we fail to reject $H_0$ at significance level $\alpha$. 

## Anatomy of a hypothesis test

:::{.noteblock}
To summarize, the elements of a statistical test are:  

- Null hypothesis \(H_0\)
- Alternative hypothesis \(H_a\)
- Test statistic 
- Significance level \(\alpha\) / critical value \(c_\alpha\)
- \(P\)-value / Rejection region \(R\)
:::  

## Null distribution {.columns-2 .vcenter}

![](images/htests.png){width=100% alt="Pictoral representation of null distribution, with level and rejection region."}

E.g., consider hypothesis tests for a population mean. 

- Distribution for \(\bar{X}\)
- Under the assumption that \(H_0\) is true, so curve is centered at \(\mu_0\)
- Level \(\alpha\) is a probability (area)
- Critical value \(c\) is a point
- Test statistic?


## {.mySegue .flexbox .vcenter}
Why 0.05?

## Emily Dickinson and monkeys on the stair {.smaller}

![](images/Batu_3.jpg){width=75% .left alt="Sarah Lee, Wikimedia Commons, CC BY-SA 4.0."}

Sarah Lee, Wikimedia Commons, CC-BY-SA 4.0

## Personal significance level

What is our MA22004 personal significance level?

[www.openintro.org/book/stat/why05/](https://www.openintro.org/book/stat/why05/)

## {.mySegue .flexbox .vcenter}
Inferences for means


## Inferences for means

### Confidence intervals 

\[\text{point estimate}  \pm (\text{critical value}) \cdot (\text{precision of point estimate})\]

### Hypothesis tests

A hypothesis test asks if the available data provides sufficient evidence to reject a null hypothesis $H_0$ which is assume to be true. 

Tests on equality of mean $\mu$, i.e., $H_0 : \mu = \mu_0$ against alternatives

- $H_a : \mu \neq \mu_0$
- $H_a : \mu > \mu_0$
- $H_a : \mu < \mu_0$

<div class="notes">
- Note the equality is part of the null hypothesis.
- If the observations disagree with $H_0$, then we reject the null hypothesis. 
- If the sample evidence does not strongly contradict $H_0$, then we continue to believe $H_0$. 
- $\mu_0$ is the **null value** that separate the null and alternative
</div>

## Estimating means: consider the population

We consider inferences for three different parent populations. 

1. Normal population with known $\sigma$
2. Any population with unknown $\sigma$, when $m$ large
3. Normal population with unknown $\sigma$, $m$ may be small

What changes? What stays the same?

## Estimating means: $100(1-\alpha)\%$ CIs

1. Normal population with known $\sigma$:
\[\overline{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{m}}\]
2. Any population with unknown $\sigma$, when $m$ large
\[\overline{x} \pm z_{\alpha/2} \frac{s}{\sqrt{m}}\]
3. Normal population with unknown $\sigma$, $m$ may be small
\[\overline{x} \pm t_{\alpha/2, m-1} \frac{s}{\sqrt{m}}\]


## Estimating means: hypothesis tests

1. Normal population with known $\sigma$:
\[Z = \frac{\overline{X} - \mu_0}{\sigma / \sqrt{m}} \sim \mathsf{N}(0,1)\]
2. Any population with unknown $\sigma$, when $m$ large
\[Z = \frac{\overline{X} - \mu_0}{S / \sqrt{m}} \sim \mathsf{N}(0,1)\]
3. Normal population with unknown $\sigma$, $m$ may be small
\[T = \frac{\overline{X} - \mu_0}{S / \sqrt{m}} \sim \mathsf{t}(m-1)\]

<div class="notes">
- Once we have sample data we make point estimate!
</div>


## The skinny on hypothesis testing

:::{.warningblock}
Two ways to conclude a hypothesis test.   
But both really the same. 
:::

### One way: 

1. Compute the value of an appropriate test statistic.
2. Determine the $P$-value, probability calculated assuming the null is true of observing a test statistic value at least as contradictory to $H_0$ as what was obtained from evidence.
3. For given $\alpha$, reject $H_0$ if evidence is strong i.e. $P < \alpha$. If evidence is not strong enough, we fail to reject $H_0$. 

## The skinny on hypothesis testing

:::{.warningblock}
Two ways to conclude a hypothesis test.   
But both really the same. 
:::

### Another way: 

1. Compute the value of an appropriate test statistic.
2. Determine the critical value for the level $\alpha$ and hence identify the rejection region for the hypothesis test.
3. If statistic falls in rejection region, reject $H_0$ because evidence is strong. If statistic falls outwith rejection region, we fail to reject $H_0$ because evidence is not strong enough. 

## Example (hypothesis testing 1/n)

### Freshman 15 [Devore eg 8.2]

> *A common belief among the lay public is that body weight increases after entry into [university], and the phrase "freshman 15" has been coined to describe the 15 pounds that students presumably gain over their freshman [(i.e. first)] year.*

## Example (2/n)

Let $\mu$ denote the true average weight gain of women over the course of their first year at university. The foregoing quote suggests that we should test the hypotheses $H_0: \mu = 15$ versus $H_a : \mu \neq 15$. 

Suppose that a random sample of $m$ such individuals is selected and the weight gain of each one is determined, resulting in a sample mean weight gain $\overline{x}$ and a sample standard deviation $s$. 

<div class="notes">
- Before data is obtained, the sample mean weight gain is a random variable $X$ and the sample standard deviation is also a random variable $S$!
</div>

## Example (3/n)

<div class="notes">
- \[Z = \frac{\overline{X} - \mu}{\sigma/ \sqrt{m}}\]
- Assuming $H_0$ is true: \[Z = \frac{\overline{X} - 15}{\sigma/ \sqrt{m}}\]
- But don't know $\sigma$:
\[Z = \frac{\overline{X} - 15}{S/ \sqrt{m}} \sim \mathsf{N}(0,1)\]
</div>

## Example (4/n)

Suppose $\overline{x} = 13.7$ and plugging in values for $s$ and $\sqrt{n}$ yields $z = -2.8$. 

Which values of the test statistic are at least as contradictory to $H_0$ as $-2.80$ itself?

<div class="notes">
- E.g., $m = 40$ and $s = 2.936401$
- Possible values: 13.5, 13.0, any value smaller than 13.7 is more contradictory to $H_0$ than 13.7
- 16.3 is just as contradictory to $H_0$ as is 13.7; both fall the same distance from the null value 15
- Just as values of $\overline{x}$ that are at most 13.7 correspond to $z\leq -2.8$, values of $\overline{x}$ that are at least 16.3 correspond to $z \geq 2$
</div>

## Example (5/n)

$P( Z \leq -2.80 \quad \text{or}\quad  Z \geq 2.80 \quad \text{assuming } H_0 \text{ true})$

<div class="notes">
$\approx 2 \cdot$ (area under the $z$ curve ot the right of 2.80)
$= 2[1 - \Phi(2.80)] = 2[1 - 0.9974] = 0.0052

- If the null hypothesis is in fact true, only about one half of one percent of all samples would result in a test statistic value at least as contradictory to the null hypothesis as is our value. Clearly -2.80 is among the possible test statistic values that are most contradictory to $H_0$. 
- The evidence suggest to reject $H_0$.
</div>

## Example (6/n)

The article *Freshman 15: Fact or Fiction* (Obesity, 2006: 1438â€“ 1443), for $m = 137$ students, the $\overline{x} = 2.42$ lb with $s =  5.72$ lb. 

This gives \[z = (2.42 - 15)/(5.72 /\sqrt{137}) = -25.7\,.\]

The probability of observing a value at least this extreme in either direction is essentially zero! 

The data very strongly contradicts the null hypothesis, and there is substantial evidence that true average weight gain is much closer to $0$ than to $15$.

<div class="notes">
- Note this implies some students lost weight!
</div>


## Summary

- Today we discussed hypothesis tests and engaged in an activity to understand "Why 0.05?"
- We discussed inferences for population means. 









